{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FM_SudipNeupane_2_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gabGpWZ76d7f",
        "colab_type": "text"
      },
      "source": [
        "Sudip Neupane\n",
        "\n",
        "Assignmnet 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mt6Rc6fvjJeT",
        "colab_type": "text"
      },
      "source": [
        "this model is built to change the corpus of the text. the corpous is used in used for research, scholarship, and teaching. \n",
        "\n",
        "\n",
        "\n",
        "Refernce :\n",
        "https://www.thoughtco.com/what-is-corpus-language-1689806"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEb5JKfwv2tP",
        "colab_type": "text"
      },
      "source": [
        "Loading necessary libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEUvtE26jwcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccfgnKMVv2ro",
        "colab_type": "text"
      },
      "source": [
        "prevent version of tensorflow is 1.x so checking the version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmPzBJaokBD_",
        "colab_type": "code",
        "outputId": "db58e9ab-3a34-4f37-b67c-c9273d113be0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCLIFzfnv2ou",
        "colab_type": "text"
      },
      "source": [
        "We are trying to use tensorflow version 2.0.0\n",
        "so we need to install tf 2.x\n",
        "\n",
        "for this, first uninstalling tf 1.15.0 \n",
        "and reinstalling the tf 2.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNPSATuRktYv",
        "colab_type": "text"
      },
      "source": [
        "First uninstalling tf 1.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0qSIZjykd4x",
        "colab_type": "code",
        "outputId": "4d7ad8d1-4ab4-48bb-e217-6a31f9e2bce0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "!pip uninstall tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.15.0:\n",
            "  Would remove:\n",
            "    /usr/local/bin/estimator_ckpt_converter\n",
            "    /usr/local/bin/freeze_graph\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/bin/tf_upgrade_v2\n",
            "    /usr/local/bin/tflite_convert\n",
            "    /usr/local/bin/toco\n",
            "    /usr/local/bin/toco_from_protos\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-1.15.0.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow_core/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDXvxoyrk4zg",
        "colab_type": "text"
      },
      "source": [
        "Now, Reinstalling tf 2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Inamirsk9_Q",
        "colab_type": "code",
        "outputId": "7b08e897-d0e9-4e1d-d129-828a206ce08d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.17.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 51.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (41.6.0)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/cb/786dc53d93494784935a62947643b48250b84a882474e714f9af5e1a1928/google_auth-1.7.1-py2.py3-none-any.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.7)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.7)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.7.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google-auth, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed google-auth-1.7.1 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_core",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-29ofR3bv2mH",
        "colab_type": "text"
      },
      "source": [
        "Now, Checking Version of tf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uY4HLvylaoI",
        "colab_type": "code",
        "outputId": "5ba05046-2949-470d-fa32-0523dd2bd22b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvk_kVo_v2jj",
        "colab_type": "text"
      },
      "source": [
        "Again improting modules and libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xpKeaMVmP2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-_HIrN8v0jc",
        "colab_type": "text"
      },
      "source": [
        "loading data files \n",
        "File name :  \n",
        "\n",
        "                                 DRACULA\n",
        "                                 \n",
        "                                  _by_\n",
        "                              Bram Stoker\n",
        "\n",
        "data Source : http://www.gutenberg.org/files/345/345.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kS4bvfnBv1pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file='345.txt'\n",
        "url='https://www.gutenberg.org/files/345/345.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxt-gV1Pozvv",
        "colab_type": "text"
      },
      "source": [
        "Now Setting Keras for the file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luOcye29oLGK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = tf.keras.utils.get_file(file,url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToN1ZoIvpEvf",
        "colab_type": "text"
      },
      "source": [
        "Now \n",
        "Opening and reading the file \n",
        "and counting the words of file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0Va-G8zpC0e",
        "colab_type": "code",
        "outputId": "59c6513c-9a91-4ecc-a460-307d8faa646b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "text = open(path).read()\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 867184 characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gcg0akLQpiEm",
        "colab_type": "text"
      },
      "source": [
        "As we dont need all the character here\n",
        "we are taking certain strip only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSg179IQpRoA",
        "colab_type": "code",
        "outputId": "3ccc9fa7-998b-49e0-d0bc-4ef8f03f83ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# strip off text we don't need\n",
        "text = text[25500:]\n",
        "\n",
        "#the first 300 characters in text\n",
        "print(text[:299])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t it was commonly believed that on a\n",
            "certain night of the year--last night, in fact, when all evil spirits\n",
            "are supposed to have unchecked sway--a blue flame is seen over any place\n",
            "where treasure has been concealed. \"That treasure has been hidden,\" he\n",
            "went on, \"in the region through which you came l\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46pwRzJ7tLdJ",
        "colab_type": "text"
      },
      "source": [
        "Now checking the nos of unique character on the above loaded texts\n",
        "and  sorting them according to their ASCII Character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uCJfejNp167",
        "colab_type": "code",
        "outputId": "f83b9835-13e9-4c28-d672-20751a9177b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# The unique characters in the file\n",
        "vocabulary = sorted(set(text))\n",
        "print ('{} unique characters.'.format(len(vocabulary)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "86 unique characters.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgPZzHw4tlp-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Now Lets Create a dictionary where these character are keys and successive integers are their values.\n",
        "\n",
        "This is so we can find the index, meaning the numerical value for any given character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw4qSvEpteEr",
        "colab_type": "code",
        "outputId": "1d9574c6-8bb3-49ec-f1b3-fea22c007956",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Create a  dictionary of unique character keys to index values\n",
        "char_to_index = {char:index for index, char in enumerate(vocabulary)}\n",
        "print(char_to_index)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '$': 4, '%': 5, '&': 6, \"'\": 7, '(': 8, ')': 9, '*': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17, '3': 18, '4': 19, '5': 20, '6': 21, '7': 22, '8': 23, '9': 24, ':': 25, ';': 26, '=': 27, '>': 28, '?': 29, '@': 30, 'A': 31, 'B': 32, 'C': 33, 'D': 34, 'E': 35, 'F': 36, 'G': 37, 'H': 38, 'I': 39, 'J': 40, 'K': 41, 'L': 42, 'M': 43, 'N': 44, 'O': 45, 'P': 46, 'Q': 47, 'R': 48, 'S': 49, 'T': 50, 'U': 51, 'V': 52, 'W': 53, 'X': 54, 'Y': 55, 'Z': 56, '_': 57, 'a': 58, 'b': 59, 'c': 60, 'd': 61, 'e': 62, 'f': 63, 'g': 64, 'h': 65, 'i': 66, 'j': 67, 'k': 68, 'l': 69, 'm': 70, 'n': 71, 'o': 72, 'p': 73, 'q': 74, 'r': 75, 's': 76, 't': 77, 'u': 78, 'v': 79, 'w': 80, 'x': 81, 'y': 82, 'z': 83, '{': 84, '}': 85}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAj3rsMnuDcY",
        "colab_type": "text"
      },
      "source": [
        "Now sorting them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0pnP3D1t-1A",
        "colab_type": "code",
        "outputId": "65dc9125-32be-417c-b5a1-1ad0c6ce105d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "index_to_char = np.array(vocabulary)\n",
        "print(index_to_char)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n' ' ' '!' '\"' '$' '%' '&' \"'\" '(' ')' '*' ',' '-' '.' '/' '0' '1' '2'\n",
            " '3' '4' '5' '6' '7' '8' '9' ':' ';' '=' '>' '?' '@' 'A' 'B' 'C' 'D' 'E'\n",
            " 'F' 'G' 'H' 'I' 'J' 'K' 'L' 'M' 'N' 'O' 'P' 'Q' 'R' 'S' 'T' 'U' 'V' 'W'\n",
            " 'X' 'Y' 'Z' '_' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i' 'j' 'k' 'l' 'm' 'n'\n",
            " 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' '{' '}']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE76Zqz6vCsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJeD9IaPuPi2",
        "colab_type": "text"
      },
      "source": [
        "now these character can be translated into an array "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgIm5FnpujSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_as_int = np.array([char_to_index[char] for char in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njICUbgpu8py",
        "colab_type": "code",
        "outputId": "43b7309f-a1bf-4c0a-8875-450003750511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char_to_index, range(20)):\n",
        "    print(' {:4s}: {:3d},'.format(repr(char), char_to_index[char]))\n",
        "print(' ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            " '\\n':   0,\n",
            " ' ' :   1,\n",
            " '!' :   2,\n",
            " '\"' :   3,\n",
            " '$' :   4,\n",
            " '%' :   5,\n",
            " '&' :   6,\n",
            " \"'\" :   7,\n",
            " '(' :   8,\n",
            " ')' :   9,\n",
            " '*' :  10,\n",
            " ',' :  11,\n",
            " '-' :  12,\n",
            " '.' :  13,\n",
            " '/' :  14,\n",
            " '0' :  15,\n",
            " '1' :  16,\n",
            " '2' :  17,\n",
            " '3' :  18,\n",
            " '4' :  19,\n",
            " ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMYr44Slz93O",
        "colab_type": "text"
      },
      "source": [
        "plag:  \n",
        "Next, it's useful to see how the text maps to integers; here are the first few:\n",
        "\n",
        "modify.. this\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN3KmzS3ynQq",
        "colab_type": "code",
        "outputId": "b203f95a-45b2-4a69-8909-89be67e31f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Show how the first 15 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:15]), text_as_int[:15]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'t it was common' ---- characters mapped to int ---- > [77  1 66 77  1 80 58 76  1 60 72 70 70 72 71]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGmvctNt0OjE",
        "colab_type": "text"
      },
      "source": [
        "Then, we set the sentence length per input, and hence, the number of examples in an epoch of training:\n",
        "\n",
        "\n",
        "modify"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZmaeyYT0KZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5etTpwOI0cpc",
        "colab_type": "text"
      },
      "source": [
        "Next, we create data.Dataset to be used later in training:\n",
        "\n",
        "mod"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk29IGeG0UaL",
        "colab_type": "code",
        "outputId": "01f30bb8-7d2a-4e79-f98a-d2ae97f58b3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "# Display , sanity check\n",
        "for char in char_dataset.take(5):\n",
        "  print(index_to_char[char.numpy()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t\n",
            " \n",
            "i\n",
            "t\n",
            " \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnSu8aum0pbi",
        "colab_type": "text"
      },
      "source": [
        "We need to batch this data for feeding to our RNN, so we do this next:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFA8ZNxH0lOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(sequence_length+1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zs1wtC_09qe",
        "colab_type": "text"
      },
      "source": [
        "As we have set sequence_length = 100, \n",
        "so the number of characters in a batch is 101.\n",
        "\n",
        "\n",
        "Also \n",
        "have a function to create our input data and target data (required output).\n",
        "\n",
        "The function returns the text that we have been working with, together with the same text, but shifted one character along, that is, if the first word is Python and sequence_length = 5, the function returns Pytho and ython.\n",
        "\n",
        "Lets create our dataset by joining the input and output character sequences:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFH5grA30-YO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "   input_text = chunk[:-1]\n",
        "   target_text = chunk[1:]\n",
        "   return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPLvuPT51RWu",
        "colab_type": "text"
      },
      "source": [
        "We use the dataset created previously to display the input and target data.\n",
        "\n",
        "Note that the dataset.take(n)  method returns n batches from the dataset.\n",
        "\n",
        "Also, \n",
        "since we have eager execution enabled [default],\n",
        "\n",
        "\n",
        " we can use the numpy() method to find the value of a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U_Wj9GA1SUR",
        "colab_type": "code",
        "outputId": "c67ee573-4344-4192-c6a0-3f2b86c97f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        " print ('Input data: ', repr(''.join(index_to_char[input_example.numpy()]))) \n",
        " print ('Target data:', repr(''.join(index_to_char[target_example.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  't it was commonly believed that on a\\ncertain night of the year--last night, in fact, when all evil s'\n",
            "Target data: ' it was commonly believed that on a\\ncertain night of the year--last night, in fact, when all evil sp'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoSHF6nI1nT0",
        "colab_type": "text"
      },
      "source": [
        "We can now show the input and expected output for a few steps:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onlpuk_O1VIb",
        "colab_type": "code",
        "outputId": "0abe4cde-1ec7-4ab5-c05a-1abe578613ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for char, (input_index, target_index) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(char))\n",
        "    print(\" input: {} ({:s})\".format(input_index, repr(index_to_char[input_index])))\n",
        "    print(\" expected output: {} ({:s})\".format(target_index, repr(index_to_char[target_index])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            " input: 72 ('o')\n",
            " expected output: 63 ('f')\n",
            "Step    1\n",
            " input: 63 ('f')\n",
            " expected output: 1 (' ')\n",
            "Step    2\n",
            " input: 1 (' ')\n",
            " expected output: 77 ('t')\n",
            "Step    3\n",
            " input: 77 ('t')\n",
            " expected output: 65 ('h')\n",
            "Step    4\n",
            " input: 65 ('h')\n",
            " expected output: 62 ('e')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aregoHzP1uTd",
        "colab_type": "text"
      },
      "source": [
        "Next, we had set the things up for training, as follows:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk0r6ECv1491",
        "colab_type": "code",
        "outputId": "11f05d9b-690b-4ac8-f871-151a8edfc6d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# how many characters in a batch\n",
        "batch = 64\n",
        "\n",
        "# the number of training steps taken in each epoch\n",
        "steps_per_epoch = examples_per_epoch//batch # note integer division\n",
        "\n",
        "# TF data maintains a buffer in memory in which to shuffle data \n",
        "# since it is designed to work with possibly endless data\n",
        "buffer = 10000\n",
        "\n",
        "dataset = dataset.shuffle(buffer).batch(batch, drop_remainder=True)\n",
        "\n",
        "# call repeat() on dataset so data can be re-fed into the model from the beginning\n",
        "dataset = dataset.repeat()\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<RepeatDataset shapes: ((64, 64, 100), (64, 64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62wgume82K9T",
        "colab_type": "text"
      },
      "source": [
        "This gives the following dataset structure:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVj6OqDc2Ntv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bxKc2IJ2Y74",
        "colab_type": "text"
      },
      "source": [
        "Here, 64 is the batch size and 100 is the sequence length. The following are some values we need for the training:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmwRUoLE2dvK",
        "colab_type": "code",
        "outputId": "a3d16fed-0796-43f1-8e57-c8f622ad43eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# The vocabulary length in characters\n",
        "vocabulary_length = len(vocabulary)\n",
        "print (vocabulary_length)\n",
        "# The embedding dimension \n",
        "embedding_dimension = 256\n",
        "print (embedding_dimension)\n",
        "# The number of recurrent neural network units\n",
        "recurrent_nn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "86\n",
            "256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb9snQ0i2hBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}